{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"sourceType":"competition"},{"sourceId":282742,"sourceType":"modelInstanceVersion","modelInstanceId":239467,"modelId":222398}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !uv pip install kagglehub more_itertools torch transformers\nimport time","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os\n\ndef is_submission():\n    return os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n\npath = f\"/kaggle/input/jigsaw-agile-community-rules/{'test.csv' if is_submission() else 'train.csv'}\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv(path)\ndata.head(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Steps for initial solution\n\n- Step - 1 Add a column for expanding the rule by describing the given rule and make it understandable along with the prompt.\n- Step - 2 Each row updated with prompt with positive and negative examples. \n- Step - 3 Then finally setup system prompt by describing the nature of the tool and get the otuput as voilation or nonvoilation along with other information.\n            ","metadata":{}},{"cell_type":"code","source":"import kagglehub\nimport more_itertools\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForCausalLM","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hugging_face_model_name=\"google/gemma-3/transformers/gemma-3-1b-it\"\nGEMMA_PATH = kagglehub.model_download(hugging_face_model_name)\nprocessor = AutoTokenizer.from_pretrained(GEMMA_PATH)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = AutoModelForCausalLM.from_pretrained(GEMMA_PATH).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def expand_rules(rule, model, processor, device):\n\n    SYS_PROMPT_RULE = \"\"\"\n    You are an expert with understanding of community guidlines and related keywords.\n    For a given community rule provided, you go through its keywords and exapand each keyword in details. \n    \n    Output should only contain the expanded rules no explanatory text.\n    \"\"\"\n    \n    USER_PROMPT_RULE = f\"\"\"\n    Expand below rule\n    \n    {rule}\n    \n    \"\"\"\n    \n    prompt_rule_text = (\n        f\"<start_of_turn>system\\n{SYS_PROMPT_RULE}<end_of_turn>\\n\"\n        f\"<start_of_turn>user\\n{USER_PROMPT_RULE}<end_of_turn>\\n\"\n        f\"<start_of_turn>model\\n\"\n    )\n\n    rule_inputs = processor(prompt_rule_text, return_tensors=\"pt\").to(device)\n    \n    output_rules = model.generate(\n        **rule_inputs,\n        max_new_tokens=300,\n        repetition_penalty=1,\n        eos_token_id=processor.eos_token_id,\n        pad_token_id=processor.eos_token_id,\n        return_dict_in_generate=True\n    )\n    \n    generated_tokens = output_rules.sequences[:, rule_inputs[\"input_ids\"].shape[-1]:]\n    decoded_text = processor.batch_decode(generated_tokens, skip_special_tokens=True)\n\n    return decoded_text[0].split(\"--\")[0]\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nrule_df = data[['rule']].drop_duplicates().reset_index(drop=True)\nrule_df['expanded_rule'] = [\n    re.sub(r'\\n{3,}', \"\", expand_rules(rule, model, processor, device)) \n    for rule in rule_df.rule\n]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = data.merge(rule_df, on=\"rule\", how=\"left\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport math\n\ndef classify_comments_in_batch(df, model, processor, device, batch_size=16):\n\n    results = []\n    num_batches = math.ceil(len(data) / batch_size)\n\n    for i in range(num_batches):\n        \n        batch = df.iloc[i*batch_size:(i+1)*batch_size]\n        prompts = []\n\n        for _, data_row in batch.iterrows():\n    \n            SYSTEM_PROMPT_CLASSIFICATION = \"\"\"\n            \n            you are a community rule voilation inspector. \n            if you are give the rule with context and target comment along with negative and positive comments you can decide if the target comment is in voilation of the specific rule.\n            you will use positive comment to understand how those comments voilates the rule and negative comments to understand why negative comments are not breaking the rules,\n            you are required to answer only one of the 2 values, 0 or 1. 0 denotes \"not voilation\" and 1 denotes \"voilation\".\n            \n            output should be given between 0 or 1. and strictly float.\n            \n            \"\"\"\n            \n            USER_PROMPT_CLASSIFICATION = f\"\"\"\n            Classify the below comment based on the rule given, \n            Rule : {data_row.expanded_rule}\n            comment = {data_row.body}\n            \n            below are the positive comment (voilation) and negative comment (non voilation) related to the comment, \n            use these commnet to better understand the above comment: \n            \n            positive comment : {data_row.positive_example_1, data_row.positive_example_2} rule violation = 1\n            negative comment : {data_row.negative_example_1, data_row.negative_example_2} rule violation = 0\n            \n            \"\"\"\n        \n            prompt_classify_text = (\n                f\"<start_of_turn>system\\n{SYSTEM_PROMPT_CLASSIFICATION}<end_of_turn>\\n\"\n                f\"<start_of_turn>user\\n{USER_PROMPT_CLASSIFICATION}<end_of_turn>\\n\"\n                f\"<start_of_turn>model\\n\"\n            )\n            \n            prompts.append(prompt_classify_text)\n\n        classifier_inputs = processor(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n\n        with torch.no_grad():\n            output_classify = model.generate(\n                **classifier_inputs,\n                max_new_tokens=1,\n                repetition_penalty=1.3,\n                eos_token_id=processor.eos_token_id,\n                pad_token_id=processor.eos_token_id,\n                return_dict_in_generate=True\n            )\n    \n        generated_tokens = output_classify.sequences[:, classifier_inputs[\"input_ids\"].shape[-1]:]\n        decoded_text = processor.batch_decode(generated_tokens, skip_special_tokens=True)\n\n        results.extend([d.split(\"--\")[0] for d in decoded_text])\n        del classifier_inputs, output_classify, generated_tokens\n        torch.cuda.empty_cache()\n        \n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_proba = classify_comments_in_batch(data, model, processor, device, batch_size=16)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(pred_proba)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df =  data[['row_id']].copy()\ndf['rule_violation'] = pred_proba\ndf.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
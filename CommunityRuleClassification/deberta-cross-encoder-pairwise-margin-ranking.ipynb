{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DeBERTa Cross-Encoder Pairwise (Margin-Ranking) Notebook\n# --------------------------------------------------------\n# Purpose: Train a DeBERTa-based cross-encoder that, given (rule + target_comment) and an example\n# (positive OR negative), produces a scalar score. We train with a margin ranking loss so that\n# score(target, positive) > score(target, negative) + margin.\n#\n# The notebook reads train.csv, converts each row into multiple (text_a, pos_example, neg_example)\n# triples, trains the model, and saves the trained model/tokenizer into `saved_model/` which can be\n# re-used in a Kaggle notebook via AutoModelForSequenceClassification.from_pretrained('saved_model').\n#\n# Requirements:\n#   pip install transformers datasets accelerate sentencepiece\n#\n# Notes:\n# - This is a single-file runnable script / notebook cell collection. You can paste into a Kaggle\n#   notebook cell or run locally.\n# - Adjust `MODEL_NAME`, `MAX_LEN`, `BATCH_SIZE`, and `NUM_EPOCHS` for your environment.\n\nimport os\nimport random\nimport math\nfrom dataclasses import dataclass\nfrom typing import List, Dict\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    get_linear_schedule_with_warmup,\n)\n\n# -----------------------\n# Config\n# -----------------------\nMODEL_NAME = \"microsoft/deberta-v3-base\"  # good balance; change to smaller/larger as needed\nMAX_LEN = 256\nBATCH_SIZE = 16\nNUM_EPOCHS = 3\nLEARNING_RATE = 3e-5\nWEIGHT_DECAY = 0.01\nWARMUP_STEPS = 100\nMARGIN = 0.5\nSAVE_DIR = \"saved_model\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEED = 42\n\n# -----------------------\n# Utilities\n# -----------------------\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(SEED)\n\n# -----------------------\n# Read CSVs\n# -----------------------\n# Expecting files 'train.csv' and 'test.csv' in working dir. Adjust paths if different.\ntrain_path = \"train.csv\"\ntest_path = \"test.csv\"\n\nif not os.path.exists(train_path):\n    raise FileNotFoundError(f\"train.csv not found at {train_path}. Please upload train.csv to working dir\")\n\ntrain_df = pd.read_csv(train_path)\nprint(\"Train rows:\", len(train_df))\n\n# Optional: read test if available (for final inference)\nif os.path.exists(test_path):\n    test_df = pd.read_csv(test_path)\n    print(\"Test rows:\", len(test_df))\nelse:\n    test_df = None\n    print(\"test.csv not found: will only train and save model\")\n\n# Basic cleaning: ensure example columns exist and fill NaNs\nfor c in [\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\", \"rule\", \"body\"]:\n    if c not in train_df.columns:\n        raise ValueError(f\"Expected column '{c}' in train.csv\")\n    train_df[c] = train_df[c].fillna(\"\")\n\n# -----------------------\n# Build pairwise training triples\n# -----------------------\n# For each original row, we will create multiple samples each containing:\n#   text_a = f\"Rule: {rule} || Target: {body}\"\n#   pos_example (one of positives)\n#   neg_example (one of negatives)\n# This yields multiple positive/negative pairings per original row.\n\ndef build_triplets(df: pd.DataFrame) -> pd.DataFrame:\n    rows = []\n    for _, r in df.iterrows():\n        rule = r['rule']\n        body = r['body']\n        text_a = f\"Rule: {rule} || Target: {body}\"\n        pos_list = [r['positive_example_1'], r['positive_example_2']]\n        neg_list = [r['negative_example_1'], r['negative_example_2']]\n        # filter empty\n        pos_list = [p for p in pos_list if str(p).strip()]\n        neg_list = [n for n in neg_list if str(n).strip()]\n        if len(pos_list) == 0 or len(neg_list) == 0:\n            # skip rows without examples on both sides\n            continue\n        for p in pos_list:\n            for n in neg_list:\n                rows.append({\"text_a\": text_a, \"pos\": p, \"neg\": n})\n    return pd.DataFrame(rows)\n\ntriplets_df = build_triplets(train_df)\nprint(\"Triplets rows:\", len(triplets_df))\n\n# If too many triplets, sample to keep training manageable. (Optional)\nMAX_TRIPLETS = None  # e.g. 200_000\nif MAX_TRIPLETS and len(triplets_df) > MAX_TRIPLETS:\n    triplets_df = triplets_df.sample(MAX_TRIPLETS, random_state=SEED).reset_index(drop=True)\n\n# -----------------------\n# Dataset & DataLoader\n# -----------------------\n@dataclass\nclass Collator:\n    tokenizer: AutoTokenizer\n    max_len: int\n\n    def __call__(self, batch: List[Dict]):\n        # batch is list of dicts with keys text_a, pos, neg\n        text_a_list = [b['text_a'] for b in batch]\n        pos_list = [b['pos'] for b in batch]\n        neg_list = [b['neg'] for b in batch]\n        # Tokenize pairs in batch\n        pos_enc = self.tokenizer(text_a_list, pos_list, padding=True, truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n        neg_enc = self.tokenizer(text_a_list, neg_list, padding=True, truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n        return {\n            'pos_input_ids': pos_enc['input_ids'],\n            'pos_attention_mask': pos_enc['attention_mask'],\n            'neg_input_ids': neg_enc['input_ids'],\n            'neg_attention_mask': neg_enc['attention_mask'],\n        }\n\nclass TripletDataset(Dataset):\n    def __init__(self, df: pd.DataFrame):\n        self.df = df.reset_index(drop=True)\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        return {\"text_a\": row['text_a'], \"pos\": row['pos'], \"neg\": row['neg']}\n\n# -----------------------\n# Model & tokenizer\n# -----------------------\nprint(\"Loading tokenizer and model...\", MODEL_NAME)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=1)  # scalar score\nmodel.to(DEVICE)\n\n# -----------------------\n# Dataloaders\n# -----------------------\ndataset = TripletDataset(triplets_df)\ncollator = Collator(tokenizer=tokenizer, max_len=MAX_LEN)\n\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator, drop_last=True)\n\n# -----------------------\n# Training loop (MarginRankingLoss)\n# -----------------------\noptim = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\ntotal_steps = len(dataloader) * NUM_EPOCHS\nscheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps)\n\nloss_fn = torch.nn.MarginRankingLoss(margin=MARGIN)\n\nprint(f\"Starting training on device={DEVICE}, steps={total_steps}\")\nmodel.train()\n\nfor epoch in range(NUM_EPOCHS):\n    running_loss = 0.0\n    for step, batch in enumerate(dataloader):\n        pos_input_ids = batch['pos_input_ids'].to(DEVICE)\n        pos_attention_mask = batch['pos_attention_mask'].to(DEVICE)\n        neg_input_ids = batch['neg_input_ids'].to(DEVICE)\n        neg_attention_mask = batch['neg_attention_mask'].to(DEVICE)\n\n        # Forward pass: compute scalar logits for pos and neg\n        outputs_pos = model(input_ids=pos_input_ids, attention_mask=pos_attention_mask)\n        logits_pos = outputs_pos.logits.view(-1)  # shape (B,)\n\n        outputs_neg = model(input_ids=neg_input_ids, attention_mask=neg_attention_mask)\n        logits_neg = outputs_neg.logits.view(-1)\n\n        # For MarginRankingLoss, target=1 means pos should be ranked higher than neg\n        target = torch.ones_like(logits_pos).to(DEVICE)\n        loss = loss_fn(logits_pos, logits_neg, target)\n\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n        scheduler.step()\n\n        running_loss += loss.item()\n        if (step + 1) % 50 == 0:\n            avg_loss = running_loss / 50\n            print(f\"Epoch {epoch+1}/{NUM_EPOCHS} step {step+1}/{len(dataloader)} avg_loss={avg_loss:.4f}\")\n            running_loss = 0.0\n\n    # end epoch\n    print(f\"Epoch {epoch+1} completed\")\n\n# -----------------------\n# Save model & tokenizer\n# -----------------------\nos.makedirs(SAVE_DIR, exist_ok=True)\nprint(f\"Saving model to {SAVE_DIR}\")\nmodel.save_pretrained(SAVE_DIR)\ntokenizer.save_pretrained(SAVE_DIR)\n\n# -----------------------\n# Inference utility\n# -----------------------\nimport math\nfrom tqdm.auto import tqdm\n\ndef sigmoid(x):\n    return 1.0 / (1.0 + math.exp(-x))\n\n# Score single pair (rule+target, example) -> scalar probability 0..1\n@torch.no_grad()\ndef score_pair(model, tokenizer, rule, target, example, device=DEVICE, max_len=MAX_LEN):\n    text_a = f\"Rule: {rule} || Target: {target}\"\n    text_b = f\"Example: {example}\"\n    enc = tokenizer(text_a, text_b, truncation=True, padding=True, max_length=max_len, return_tensors=\"pt\").to(device)\n    out = model(**enc)\n    logit = out.logits.squeeze().cpu().item()\n    return sigmoid(logit)\n\n# If test.csv available, run inference\nif test_df is not None:\n    # Ensure test has example columns; if not, you'll need to provide examples for new rules during inference\n    for c in [\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\", \"rule\", \"body\"]:\n        if c not in test_df.columns:\n            test_df[c] = \"\"\n    test_df[\"positive_example_1\"] = test_df[\"positive_example_1\"].fillna(\"\")\n    test_df[\"positive_example_2\"] = test_df[\"positive_example_2\"].fillna(\"\")\n    test_df[\"negative_example_1\"] = test_df[\"negative_example_1\"].fillna(\"\")\n    test_df[\"negative_example_2\"] = test_df[\"negative_example_2\"].fillna(\"\")\n\n    preds = []\n    print(\"Running inference on test set (this may be slow because each test row uses multiple forward passes)...\")\n    for _, r in tqdm(test_df.iterrows(), total=len(test_df)):\n        rule = r['rule']\n        body = r['body']\n        pos_list = [r['positive_example_1'], r['positive_example_2']]\n        neg_list = [r['negative_example_1'], r['negative_example_2']]\n        pos_list = [p for p in pos_list if str(p).strip()]\n        neg_list = [n for n in neg_list if str(n).strip()]\n        if len(pos_list) == 0 or len(neg_list) == 0:\n            preds.append(0.0)\n            continue\n        pos_scores = [score_pair(model, tokenizer, rule, body, p) for p in pos_list]\n        neg_scores = [score_pair(model, tokenizer, rule, body, n) for n in neg_list]\n        pos_avg = sum(pos_scores) / len(pos_scores)\n        neg_avg = sum(neg_scores) / len(neg_scores)\n        # A reasonable final probability: sigmoid(pos_avg - neg_avg) but pos_avg and neg_avg are already prob\n        final_prob = 1.0 / (1.0 + math.exp(- (pos_avg - neg_avg) * 8.0))  # scale factor to make differences sharper\n        preds.append(final_prob)\n\n    test_df['pred_rule_violation_prob'] = preds\n    out_path = 'test_with_preds.csv'\n    test_df.to_csv(out_path, index=False)\n    print(f\"Saved predictions -> {out_path}\")\n\n# -----------------------\n# How to reuse saved model in Kaggle\n# -----------------------\nprint('\\nTo reuse the saved model in a Kaggle notebook:')\nprint('from transformers import AutoModelForSequenceClassification, AutoTokenizer')\nprint(\"model = AutoModelForSequenceClassification.from_pretrained('saved_model')\")\nprint(\"tokenizer = AutoTokenizer.from_pretrained('saved_model')\")\nprint(\"# then use score_pair or your own inference loop\")\n\nprint('Done')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}